{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cPickle\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "%pylab inline \n",
    "\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format = u'[%(asctime)s]  %(message)s', level = logging.INFO)\n",
    "\n",
    "\n",
    "def calc_entropy(X):\n",
    "    ent = np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        x = X[i]*1.0 / (sum(X[i]) + 0.00001)\n",
    "        ent[i] = -np.sum(x*np.log(x+0.00000001))\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainLabels = pd.read_csv('../../dima/data/trainLabels.csv')\n",
    "sampleSubmission = pd.read_csv('../../malware/data/raw/sampleSubmission.csv')\n",
    "#print trainLabels.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import set_up\n",
    "feats_path = '../data/feats/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lines features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2015-04-29 20:51:23,392]  Collect sections...\n"
     ]
    }
   ],
   "source": [
    "logging.info('Collect sections...')\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "section_whitelist = set(['.bss', '.data', '.edata', '.idata', '.rdata', '.reloc',\n",
    "                       '.rsrc', '.text', '.tls', 'bss', 'code', 'data', 'header'])\n",
    "\n",
    "y_tr = np.zeros(len(trainLabels))\n",
    "lines_tr = []\n",
    "\n",
    "#for train\n",
    "for i,row in trainLabels.iterrows():\n",
    "    y_tr[i] = row['Class']\n",
    "    \n",
    "    # lines\n",
    "    feat = cPickle.load(open('{}sections_hist/{}'.format(feats_path, row['Id']), 'r'))\n",
    "    del feat['sum']\n",
    "    whitened = {}\n",
    "    for x in feat:\n",
    "        if x in section_whitelist:\n",
    "            whitened[x] = feat[x]\n",
    "    lines_tr.append(whitened)\n",
    "\n",
    "        \n",
    "#for test\n",
    "lines_te = []\n",
    "for i,row in sampleSubmission.iterrows():\n",
    "    # lines\n",
    "    feat = cPickle.load(open('{}sections_hist/{}'.format(feats_path, row['Id']), 'r'))\n",
    "    del feat['sum']\n",
    "    whitened = {}\n",
    "    for x in feat:\n",
    "        if x in section_whitelist:\n",
    "            whitened[x] = feat[x]\n",
    "    lines_te.append(whitened)\n",
    "    \n",
    "# convert to matrix\n",
    "dv = DictVectorizer(sparse=False)\n",
    "dv.fit(lines_tr)\n",
    "X_lines_tr = dv.transform(lines_tr)\n",
    "X_lines_te = dv.transform(lines_te)\n",
    "\n",
    "\n",
    "E_lines_tr = calc_entropy(X_lines_tr)\n",
    "E_lines_te = calc_entropy(X_lines_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2015-04-29 20:54:56,063]  Collect file sizes\n"
     ]
    }
   ],
   "source": [
    "logging.info('Collect file sizes')\n",
    "import os\n",
    "# train\n",
    "X_sizes_tr = np.zeros([len(trainLabels), 2])\n",
    "for i,row in trainLabels.iterrows():\n",
    "    fname = row['Id']\n",
    "    X_sizes_tr[i, 0] = os.path.getsize('{}{}.bytes'.format(set_up.train_folder_path, fname))\n",
    "    X_sizes_tr[i, 1] = os.path.getsize('{}{}.asm'.format(set_up.train_folder_path, fname))\n",
    "size_ratio_tr = (X_sizes_tr[:, 0] *1.0 / X_sizes_tr[:, 1])[:, np.newaxis]\n",
    "\n",
    "#test\n",
    "X_sizes_te = np.zeros([len(sampleSubmission), 2])\n",
    "for i,row in sampleSubmission.iterrows():\n",
    "    fname = row['Id']\n",
    "    X_sizes_te[i, 0] = os.path.getsize('{}{}.bytes'.format(set_up.test_folder_path, fname))\n",
    "    X_sizes_te[i, 1] = os.path.getsize('{}{}.asm'.format(set_up.test_folder_path, fname))\n",
    "size_ratio_te = (X_sizes_te[:, 0] *1.0 / X_sizes_te[:, 1])[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spectral asm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2015-04-29 20:51:58,311]  Collect spectral asm...\n"
     ]
    }
   ],
   "source": [
    "logging.info('Collect spectral asm...')\n",
    "def read_file(filename):\n",
    "    fin = open(filename, 'r')\n",
    "    data = []\n",
    "    for line in fin:\n",
    "        data.append(line.strip())\n",
    "    return data\n",
    "\n",
    "fnames = read_file('{}spectral_asm/fnames'.format(feats_path))\n",
    "asm_dict = {}\n",
    "specter = ['jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add',\n",
    "                'imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb']\n",
    "for op in specter:\n",
    "    values = read_file('{}spectral_asm/{}'.format(feats_path, op))\n",
    "    for fname, val in zip(fnames, values):\n",
    "        asm_dict[fname] = asm_dict.get(fname, {})\n",
    "        asm_dict[fname][op] = val\n",
    "# train\n",
    "X_asm_tr = np.zeros((len(trainLabels), 22))\n",
    "for i, fname in enumerate(trainLabels.Id.values):\n",
    "    for j,op in enumerate(specter):\n",
    "        X_asm_tr[i,j] = asm_dict[fname][op]\n",
    "        \n",
    "E_asm_tr = calc_entropy(X_asm_tr)\n",
    "\n",
    "#test\n",
    "X_asm_te = np.zeros((len(sampleSubmission), 22))\n",
    "for i, fname in enumerate(sampleSubmission.Id.values):\n",
    "    for j,op in enumerate(specter):\n",
    "        X_asm_te[i,j] = asm_dict[fname][op]\n",
    "        \n",
    "E_asm_te = calc_entropy(X_asm_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# line counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2015-04-29 20:52:00,064]  Collect line counts...\n"
     ]
    }
   ],
   "source": [
    "logging.info('Collect line counts...')\n",
    "# line_counts\n",
    "fnames = read_file('{}spectral_asm/fnames'.format(feats_path))\n",
    "line_counts = read_file('{}spectral_asm/line_count'.format(feats_path))\n",
    "\n",
    "line_dict = {}\n",
    "for i, fname in enumerate(fnames):\n",
    "    line_dict[fname] = line_counts[i]\n",
    "\n",
    "# train\n",
    "X_lcounts_tr = np.zeros((len(trainLabels), 1))\n",
    "for i, fname in enumerate(trainLabels.Id.values):\n",
    "    X_lcounts_tr[i, 0] = line_dict[fname]\n",
    "    \n",
    "E_lcounts_tr = calc_entropy(X_lcounts_tr)\n",
    "\n",
    "# test\n",
    "X_lcounts_te = np.zeros((len(sampleSubmission), 1))\n",
    "for i, fname in enumerate(sampleSubmission.Id.values):\n",
    "    X_lcounts_te[i, 0] = line_dict[fname]\n",
    "    \n",
    "E_lcounts_te = calc_entropy(X_lcounts_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2015-04-29 20:52:01,275]  Collect calls...\n"
     ]
    }
   ],
   "source": [
    "logging.info('Collect calls...')\n",
    "def get_call_list(fname):\n",
    "    calls =[]\n",
    "    lines = read_file(fname)\n",
    "    for line in lines:\n",
    "        calls.append(line.split('__stdcall')[1].split('(')[0].split('_')[0].strip())\n",
    "    return calls\n",
    "\n",
    "# train\n",
    "call_txt_tr = []\n",
    "for i,row in trainLabels.iterrows():\n",
    "    call_txt_tr.append(' '.join(get_call_list('{}stdcall_grepper/'.format(feats_path) + row['Id'])))\n",
    "\n",
    "# train\n",
    "call_txt_te = []\n",
    "for i,row in sampleSubmission.iterrows():\n",
    "    call_txt_te.append(' '.join(get_call_list('{}stdcall_grepper/'.format(feats_path) + row['Id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2015-04-29 20:52:07,933]  -> vectorizing...\n"
     ]
    }
   ],
   "source": [
    "logging.info('-> vectorizing...')\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vect = TfidfVectorizer(max_features=10000)\n",
    "vect.fit(call_txt_tr + call_txt_te)\n",
    "X_call_tr = vect.transform(call_txt_tr)\n",
    "X_call_te = vect.transform(call_txt_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2015-04-29 20:52:14,599]  -> apply NMF...\n"
     ]
    }
   ],
   "source": [
    "logging.info('-> apply NMF...')\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from scipy import sparse\n",
    "\n",
    "nmf = NMF(n_components=10, sparseness='data')\n",
    "nmf.fit(sparse.vstack([X_call_tr, X_call_te]))\n",
    "X_calls_nmf_tr = nmf.transform(X_call_tr)\n",
    "X_calls_nmf_te = nmf.transform(X_call_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2015-04-29 20:53:19,187]  Collect FUNCs...\n"
     ]
    }
   ],
   "source": [
    "logging.info('Collect FUNCs...')\n",
    "\n",
    "def get_func_list(fname):\n",
    "    procs =[]\n",
    "    lines = read_file(fname)\n",
    "    for line in lines:\n",
    "        line2 = line.split('FUNCTION')[1]\n",
    "        if 'PRESS' in line2:\n",
    "            procs.append(line2.split('PRESS')[0].strip().replace('.', ''))\n",
    "    return procs\n",
    "\n",
    "func_txt_tr = []\n",
    "func_txt_te = []\n",
    "\n",
    "for i,row in trainLabels.iterrows():\n",
    "    func_txt_tr.append(' '.join(get_func_list('{}func_grepper/'.format(feats_path) + row['Id'])))\n",
    "        \n",
    "for i,row in sampleSubmission.iterrows():\n",
    "    func_txt_te.append(' '.join(get_func_list('{}func_grepper/'.format(feats_path) + row['Id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2015-04-29 20:53:25,635]  -> vectorizing...\n",
      "[2015-04-29 20:53:33,659]  -> apply NMF...\n"
     ]
    }
   ],
   "source": [
    "logging.info('-> vectorizing...')\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vect_func = TfidfVectorizer(max_features=10000)\n",
    "vect_func.fit(func_txt_tr + func_txt_te)\n",
    "\n",
    "X_func_tr = vect_func.transform(func_txt_tr)\n",
    "X_func_te = vect_func.transform(func_txt_te)\n",
    "\n",
    "logging.info('-> apply NMF...')\n",
    "nmf_func = NMF(n_components=10, sparseness='data')\n",
    "nmf_func.fit(sparse.vstack([X_func_tr, X_func_te]))\n",
    "X_func_nmf_tr = nmf_func.transform(X_func_tr)\n",
    "X_func_nmf_te = nmf_func.transform(X_func_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2015-04-29 20:54:23,732]  Build all together...\n"
     ]
    }
   ],
   "source": [
    "logging.info('Build all together...')\n",
    "X_train_tr = np.hstack((\n",
    "                          X_lines_tr, \n",
    "                          size_ratio_tr, \n",
    "                          X_asm_tr,\n",
    "                          X_sizes_tr, #!\n",
    "                          E_lines_tr[:, np.newaxis], #!\n",
    "                          X_calls_nmf_tr,\n",
    "                          X_func_nmf_tr\n",
    "                          ))\n",
    "\n",
    "X_train_te = np.hstack((\n",
    "                          X_lines_te, \n",
    "                          size_ratio_te, \n",
    "                          X_asm_te,\n",
    "                          X_sizes_te, #!\n",
    "                          E_lines_te[:, np.newaxis], #!\n",
    "                          X_calls_nmf_te,\n",
    "                          X_func_nmf_te\n",
    "                          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/feats/X_basepack',\n",
       " '../data/feats/X_basepack_01.npy',\n",
       " '../data/feats/X_basepack_02.npy']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((X_train_tr, X_train_te), '{}X_basepack'.format(feats_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2015-04-29 20:54:25,490]  Done!\n"
     ]
    }
   ],
   "source": [
    "logging.info('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
